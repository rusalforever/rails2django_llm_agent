import logging
import json
import re
from pathlib import Path
from agent.tools.log_utils import log_state, log_llm_call
from agent.state import ConversionState


class LLMPlannerNode:
    """
    –ü–æ–±—É–¥–æ–≤–∞ –ø–æ—á–∞—Ç–∫–æ–≤–æ–≥–æ –ø–ª–∞–Ω—É –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó Rails‚ÜíDjango.
    –í–∏–∫–ª–∏–∫–∞—î LLM –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–∫—Ä–æ–∫–æ–≤–æ–≥–æ –ø–ª–∞–Ω—É –¥—ñ–π –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤—Ö—ñ–¥–Ω–æ–≥–æ –ø—Ä–æ—î–∫—Ç—É.
    """

    def __init__(self, client, prompt_path: str = "agent/prompts/plan_prompt.txt"):
        self.client = client
        self.prompt_path = Path(prompt_path)

    def __call__(self, state: ConversionState):
        node = "LLMPlannerNode"
        logging.info(f"[1/6] üß≠ {node} started...")

        # --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–æ–º–ø—Ç–∞ ---
        if self.prompt_path.exists():
            prompt_template = self.prompt_path.read_text(encoding="utf-8")
        else:
            # fallback-–ø—Ä–æ–º–ø—Ç, —è–∫—â–æ —Ñ–∞–π–ª–∞ –Ω–µ–º–∞—î
            logging.warning(f"‚ö†Ô∏è Prompt file not found at {self.prompt_path}. Using default prompt.")
            prompt_template = (
                "You are an expert AI assistant specializing in converting Ruby on Rails projects to Django. "
                "Analyze the Rails project located at '{{input_dir}}' and produce a detailed JSON plan for conversion. "
                "The JSON should include a list of steps with clear actions, descriptions, and dependencies."
            )

        # --- –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –¥–∏–Ω–∞–º—ñ—á–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ ---
        input_dir = Path(state.input_dir).resolve()
        prompt = prompt_template.replace("{{input_dir}}", str(input_dir))

        # --- –í–∏–∫–ª–∏–∫ LLM ---
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are a senior software architect."},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.2,
            )
            llm_output = response.choices[0].message.content.strip()
        except Exception as e:
            logging.error(f"‚ùå LLM API call failed: {e}")
            raise

        # --- –õ–æ–≥—É–≤–∞–Ω–Ω—è ---
        log_llm_call(node, prompt, llm_output)

        # --- –û–±—Ä–æ–±–∫–∞ —Ç–∞ –ø–∞—Ä—Å–∏–Ω–≥ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ ---
        # –≤–∏–¥–∞–ª—è—î–º–æ –∫–æ–¥-–±–ª–æ–∫–∏ ```json ... ```
        clean_output = re.sub(r"^```(json|python)?|```$", "", llm_output.strip(), flags=re.MULTILINE).strip()

        try:
            parsed = json.loads(clean_output)
        except json.JSONDecodeError:
            logging.warning("‚ö†Ô∏è Planner output is not valid JSON. Storing raw output instead.")
            parsed = {"raw_plan": clean_output}

        # --- –û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞–Ω—É ---
        state.plan = parsed
        state.llm_response = llm_output
        state.current_node = "planner"

        log_state(node, state)
